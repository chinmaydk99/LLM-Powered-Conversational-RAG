{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0WegBt5kW8B5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e062205-9d26-4d73-a43c-d614ff222928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.4/802.4 kB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.2/37.2 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.9/218.9 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m613.2/613.2 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U torch datasets  tensorflow langchain playwright html2text sentence_transformers faiss-cpu\n",
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7\n",
        "# !pip install -q -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "-5AQLLa0_tCd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U playwright"
      ],
      "metadata": {
        "id": "6VRK_V3WsZJU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "# !pip install -q datasets loralib sentencepiece\n",
        "# !pip -q install bitsandbytes accelerate xformers einops\n",
        "# !pip -q install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Iuz5JVDwyxL",
        "outputId": "0de829f0-1a7d-48c2-baab-d8aa1259ed03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary libraries"
      ],
      "metadata": {
        "id": "Wc4fmOzwrO6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import (\n",
        "  AutoTokenizer,\n",
        "  AutoModelForCausalLM,\n",
        "  BitsAndBytesConfig,\n",
        "  pipeline\n",
        ")\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "import nest_asyncio"
      ],
      "metadata": {
        "id": "FDgTRk2prVk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer"
      ],
      "metadata": {
        "id": "ufcLclCBq5Ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input sequences need to be padded so that they are of equal length. We are using the 'EOS' token to pad the sequences"
      ],
      "metadata": {
        "id": "gTEWO4C7zI1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
        "\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_name\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code = True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = 'right'"
      ],
      "metadata": {
        "id": "WBE74qwin74Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bits and Bytes parameters\n",
        "\n",
        "Optimizing model performance by adjusting precision and quantization"
      ],
      "metadata": {
        "id": "FHPyWCR4zmGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_4bit = True\n",
        "bnb_4bit_compute_dtype = 'float16'\n",
        "bnb_4bit_quant_type = 'nf4'\n",
        "use_nested_quant = False"
      ],
      "metadata": {
        "id": "VgpIeW_-u3KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting up Quantization Configuration"
      ],
      "metadata": {
        "id": "4OTEdper3_po"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = getattr(torch,bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit= use_4bit,\n",
        "    bnb_4bit_quant_type= bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype= compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n"
      ],
      "metadata": {
        "id": "SCvcJfY10M0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading pre-trained configuration"
      ],
      "metadata": {
        "id": "yH5XSQRL4lcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config = bnb_config\n",
        ")"
      ],
      "metadata": {
        "id": "czPnqtEk4iVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing out a random prompt with Mistral 7B"
      ],
      "metadata": {
        "id": "YA_aRtEW48mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and encode the inout prompt\n",
        "inputs_not_chat = tokenizer.encode_plus(\"[INST]Tell me what you know about fantasy soccer? [/INST]\", return_tensors='pt')['input_ids'].to('cuda')\n",
        "\n",
        "generated_ids = model.generate(inputs_not_chat,\n",
        "                               max_new_tokens = 1000,\n",
        "                               do_sample = True)\n",
        "\n",
        "# Convert back to human interpretable form\n",
        "decoded = tokenizer.batch_decode(generated_ids)"
      ],
      "metadata": {
        "id": "kt1F6Kv940VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded"
      ],
      "metadata": {
        "id": "68vnjG-d6fTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking number of parameters in the model"
      ],
      "metadata": {
        "id": "I67_qf-Y6y2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(model))"
      ],
      "metadata": {
        "id": "0uxe9rAt6um-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up playwright"
      ],
      "metadata": {
        "id": "5uw4KoLWAh86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlSf3NMgAkI4",
        "outputId": "028915e9-b135-4b6c-b012-72403564f2be"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.10/dist-packages (1.40.0)\n",
            "Requirement already satisfied: greenlet==3.0.1 in /usr/local/lib/python3.10/dist-packages (from playwright) (3.0.1)\n",
            "Requirement already satisfied: pyee==11.0.1 in /usr/local/lib/python3.10/dist-packages (from playwright) (11.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee==11.0.1->playwright) (4.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!playwright install"
      ],
      "metadata": {
        "id": "n3N_dYbBC5Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install html2text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x51aJ2FPElhT",
        "outputId": "32f269cc-04ad-487a-a0ff-85c34459db3d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting html2text\n",
            "  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: html2text\n",
            "Successfully installed html2text-2020.1.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "LDrIqBQNFIWj"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Vector Database"
      ],
      "metadata": {
        "id": "CQgrPw5T7pjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_transformers import Html2TextTransformer\n",
        "from langchain.document_loaders import AsyncChromiumLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Articles that we will be using for additional context\n",
        "articles = [\"https://www.fantasypros.com/2023/11/rival-fantasy-nfl-week-10/\",\n",
        "            \"https://www.fantasypros.com/2023/11/5-stats-to-know-before-setting-your-fantasy-lineup-week-10/\",\n",
        "            \"https://www.fantasypros.com/2023/11/nfl-week-10-sleeper-picks-player-predictions-2023/\",\n",
        "            \"https://www.fantasypros.com/2023/11/nfl-dfs-week-10-stacking-advice-picks-2023-fantasy-football/\",\n",
        "            \"https://www.fantasypros.com/2023/11/players-to-buy-low-sell-high-trade-advice-2023-fantasy-football/\"]\n",
        "\n",
        "# Scrape the articles\n",
        "loader = AsyncChromiumLoader(articles)\n",
        "docs = loader.load()\n",
        "\n",
        "# Converting HTML to plain text\n",
        "html2text = Html2TextTransformer()\n",
        "docs_transformed = html2text.transform_documents(docs)\n",
        "\n",
        "# Chunk text\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap = 0\n",
        ")\n",
        "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
        "\n",
        "# Loading the chunked files into the FAISS index\n",
        "db = FAISS.from_documents(chunked_documents,\n",
        "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
        "\n",
        "\n",
        "# Connect query to FAISS index using a retriever\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={'k': 4}\n",
        ")"
      ],
      "metadata": {
        "id": "ywnhxDPE64NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Tell me about Patrick Mahomes\"\n",
        "docs = db.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "Iz5wK6T6CpQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`retriever` : Acts as an interface between the vector database (where we provide additional context) and the LLM"
      ],
      "metadata": {
        "id": "lVIdH4xv7-Ak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the LLM Chain"
      ],
      "metadata": {
        "id": "TvZCfFaDUdnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "text_generation_pipeline = transformers.pipeline(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    task = 'text-generation',\n",
        "    temperature = 0.2,\n",
        "    repetition_penalty = 1.1,\n",
        "    return_full_text = True,\n",
        "    max_new_tokens =  300\n",
        ")\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "### [INST]\n",
        "Instruction: Answer the question based on your fantasy football knowledge.\n",
        "Here is some context to help:\n",
        "\n",
        "{context}\n",
        "\n",
        "### QUESTION:\n",
        "{question}\n",
        "\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "mistral_llm = HuggingFacePipeline(pipeline = text_generation_pipeline)\n",
        "\n",
        "# Creating a prompt from the prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables = [\"context\",\"question\"],\n",
        "    template = prompt_template\n",
        ")\n",
        "\n",
        "# Creating the LLM chain\n",
        "llm_chain = LLMChain(llm = mistral_llm, prompt = prompt)\n"
      ],
      "metadata": {
        "id": "YSqM4hw1HbAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So currently we have our LLMChain which doesn't have the external context data source"
      ],
      "metadata": {
        "id": "xmSW818uXqQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"context\":\"\",\n",
        "                  \"question\":\"Should I pick Alvin Kamara for my fantasy team?\"})"
      ],
      "metadata": {
        "id": "xu6QUMn0XUMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the response is pretty generic. Let's now provide it with additional context ie, we integrate our FAISS database with the llm chain"
      ],
      "metadata": {
        "id": "XFbcDdKgYH2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Should I pick up Alvin Kamara for my fantasy team?\"\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\":retriever,\n",
        "     \"question\":RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "rag_chain.invoke(query)"
      ],
      "metadata": {
        "id": "p1HP447HX9ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I have Josh Jacobs, should I trade him for Kareem Hunt?\"\n",
        "\n",
        "rag_chain.invoke(query)"
      ],
      "metadata": {
        "id": "oaZFHif5Yzao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Should I trade Saquon Barkley? What are some alternatives.\"\n",
        "\n",
        "rag_chain.invoke(query)"
      ],
      "metadata": {
        "id": "SuY5NZw8ZleF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a conversational RAG\n",
        "\n",
        "Here I will be incorporating conversation history and including a second LLM responsible for generating a standalone question that can appropriately query the vector data base"
      ],
      "metadata": {
        "id": "mc-nHSb49acX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "standalone_query_generation_pipeline = pipeline(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    task = 'text-generation',\n",
        "    temperature = 0.0,\n",
        "    repetition_penalty = 1.1,\n",
        "    return_full_text = True,\n",
        "    max_new_tokens = 1000\n",
        ")\n",
        "\n",
        "standalone_query_generation_llm = HuggingFacePipeline(pipeline = standalone_query_generation_pipeline)\n",
        "\n",
        "response_generation_pipeline = pipeline(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    task = \"text-generation\",\n",
        "    temperature = 0.2,\n",
        "    repetition_penalty = 1.1,\n",
        "    return_full_text = True,\n",
        "    max_new_tokens = 1000\n",
        ")\n",
        "\n",
        "response_generation_llm = HuggingFacePipeline(pipeline =  response_generation_pipeline)\n"
      ],
      "metadata": {
        "id": "2CY4Cs-e_ZUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key point is how `temperature` has been set to 0.2 in the response generation pipeline whereas in the standalone query generation llm it is 0.0 . This is to make sure there is little chance of hallucination and relevant context is extracted"
      ],
      "metadata": {
        "id": "-B485PrrBI7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain for Standalone Questions Generation\n",
        "\n"
      ],
      "metadata": {
        "id": "ektfUvVHHqRz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rEU7pKDDZ3D2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}